{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Metaspace Annotation Pipeline on IBM Cloud\n",
    "Experimental code to integrate Metaspace [engine](https://github.com/metaspace2020/metaspace/tree/master/metaspace/engine) with [PyWren](https://github.com/pywren/pywren-ibm-cloud) for IBM Cloud\n",
    "\n",
    "## Table of contents\n",
    "1. [Follow Setup Instructions](#setup)\n",
    "2. [Upload Data Files into IBM Cloud Object Storage](#upload)\n",
    "3. [Split Dataset into Segments](#split)\n",
    "4. [Apply Annotation to each Segment in Parallel](#annotation)\n",
    "5. [Get Results](#results)\n",
    "6. [Clean Segments Data in IBM Cloud Object Storage](#clean)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <a name=\"setup\"></a> 1. Follow Setup Instructions\n",
    "\n",
    "This notebook requires IBM Cloud Object Storage and IBM Cloud Functions\n",
    "Please follow IBM Cloud dashboard and create both services.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We need this to overcome Python notebooks limitations of too many open files\n",
    "import resource\n",
    "soft, hard = resource.getrlimit(resource.RLIMIT_NOFILE)\n",
    "print('Bebore:', soft, hard)\n",
    "\n",
    "# Raising the soft limit. Hard limits can be raised only by sudo users\n",
    "resource.setrlimit(resource.RLIMIT_NOFILE, (10000, hard))\n",
    "soft, hard = resource.getrlimit(resource.RLIMIT_NOFILE)\n",
    "print('After:', soft, hard)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%config Completer.use_jedi = False\n",
    "%matplotlib inline\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run this for DEBUG mode\n",
    "import logging\n",
    "logging.basicConfig(level=logging.DEBUG)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### IBM Cloud PyWren Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Install PyWren-IBM if needed\n",
    "import sys\n",
    "try:\n",
    "    import pywren_ibm_cloud as pywren\n",
    "except ModuleNotFoundError:    \n",
    "    !{sys.executable} -m pip install -U pywren-ibm-cloud==1.0.10\n",
    "    import pywren_ibm_cloud as pywren\n",
    "\n",
    "pywren.__version__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Copy the file `config.json.template` to `config.json` and fill in the missing values for API keys, buckets and endpoints per instructions below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = json.load(open('config.json'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### IBM Cloud Object Storage:\n",
    "\n",
    "Setup a bucket in IBM Cloud Object Storage\n",
    "\n",
    "You need an IBM COS bucket which you will use to store the input data. If you don't know of any of your existing buckets or would like like to create a new one, please navigate to your cloud resource list, then find and select your storage instance. From here, you will be able to view all your buckets and can create a new bucket in the region you prefer. Make sure you copy the correct endpoint for the bucket from the Endpoint tab of this COS service dashboard. Note: The bucket names must be unique.\n",
    "\n",
    "#### IBM Cloud Functions:\n",
    "\n",
    "Obtain the API key and endpoint to the IBM Cloud Functions service. Navigate to Getting Started > API Key from the side menu and copy the values for \"Current Namespace\", \"Host\" and \"Key\" into the config below. Make sure to add \"https://\" to the host when adding it as the endpoint."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from annotation_pipeline.utils import get_ibm_cos_client\n",
    "cos_client = get_ibm_cos_client(config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Files Setup\n",
    "\n",
    "Copy the file `input_config.json.template` to `input_config.json` and fill in the missing values for buckets.<br>\n",
    "Change `\"ds_id\"` value to use different datasets (specify one of the datasets options).<br>\n",
    "Change `\"modifiers\"` value to use different databases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_config = json.load(open('input_config.json'))\n",
    "input_data = input_config['dataset']\n",
    "input_db = input_config['molecular_db']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <a name='upload'></a> 2. Upload Data Files into IBM Cloud Object Storage\n",
    "\n",
    "### Input Dataset\n",
    "\n",
    "This part uploads input data from url or local path into IBM Cloud Object Storage.<br>\n",
    "To upload dataset from local path, define `\"ds_id\": \"local\"` and specify files' paths inside input config file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specified database to be uploaded:\n",
    "ds_id = input_data['ds_id']\n",
    "ds_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from annotation_pipeline.dataset import dumb_input_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# Downloads dataset from URL (or loads from local) and uploads (add force=True to reupload if needed)\n",
    "dumb_input_dataset(config, input_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prints details of ds.txt to ensure that everything is correct\n",
    "key=input_data['datasets'][ds_id]['ds']\n",
    "cos_client.list_objects_v2(Bucket=input_db['bucket'], Prefix=key).get('Contents', [])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prints details of ds_coords.txt to ensure that everything is correct\n",
    "key=input_data['datasets'][ds_id]['ds_coord']\n",
    "cos_client.list_objects_v2(Bucket=input_db['bucket'], Prefix=key).get('Contents', [])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate Isotopic Peaks from Molecular Databases\n",
    "\n",
    "This part creates formulas in IBM Cloud Object Storage and then genrates and uploads centroids database."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specified modifiers to be used:\n",
    "input_db['modifiers']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from annotation_pipeline.molecular_db import dump_mol_db, build_database, calculate_centroids, clean_formula_chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download commonly used mol DBs from METASPACE (add force=True to redownload if needed)\n",
    "dump_mol_db(config, input_db['bucket'], 'metabolomics/db/mol_db1.pickle', 22) #HMDB-v4\n",
    "dump_mol_db(config, input_db['bucket'], 'metabolomics/db/mol_db2.pickle', 19) #ChEBI-2018-01\n",
    "dump_mol_db(config, input_db['bucket'], 'metabolomics/db/mol_db3.pickle', 24) #LipidMaps-2017-12-12\n",
    "dump_mol_db(config, input_db['bucket'], 'metabolomics/db/mol_db4.pickle', 26) #SwissLipids-2018-02-02"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "num_formulas, formula_chunk_keys = build_database(config, input_db)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_formulas, len(formula_chunk_keys), formula_chunk_keys[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "centroids_shape, centroids_head = calculate_centroids(config, input_db, formula_chunk_keys)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "centroids_shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "centroids_head"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_formula_chunks(config, input_db, formula_chunk_keys)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prints details of centroids.pickle to ensure that everything is correct\n",
    "cos_client.list_objects_v2(Bucket=input_db['bucket'], Prefix=input_db['centroids_pandas']).get('Contents', [])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <a name='split'></a> 3. Split Dataset into Segments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from annotation_pipeline.dataset_segmentation import generate_segm_intervals, split_spectra_into_segments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "segm_n = 256"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "segm_intervals = generate_segm_intervals(config, input_db, segm_n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "segm_intervals[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "split_spectra_into_segments(config, input_data, segm_n, segm_intervals)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prints details of pickled segments\n",
    "cos_client.list_objects_v2(Bucket=input_data['bucket'], Prefix=input_data['segments']).get('Contents', [])[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prints segments number in COS\n",
    "cos_client.list_objects_v2(Bucket=input_data['bucket'], Prefix=input_data['segments'])['KeyCount']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <a name='annotation'></a> 4. Apply Annotation to each Segment in Parallel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from annotation_pipeline.annotation import annotate_spectra"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "results = annotate_spectra(config, input_data, input_db, segm_n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <a name='results'></a> 5. Get Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from annotation_pipeline.annotation import merge_annotation_results\n",
    "formula_scores_df, formula_images = merge_annotation_results(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "formula_scores_df.shape, len(formula_images)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "for key, image_set in formula_images.items():\n",
    "    img = image_set[0][1]\n",
    "    break\n",
    "plt.imshow(img.toarray())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <a name='clean'></a> 6. Clean Segments Data in IBM Cloud Object Storage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from annotation_pipeline.dataset_segmentation import clean_segments\n",
    "clean_segments(config, input_data)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}

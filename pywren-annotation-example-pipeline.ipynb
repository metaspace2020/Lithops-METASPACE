{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Initial requirements\n",
    "\n",
    "This notebook requires IBM Cloud Object Storage and IBM Cloud Functions\n",
    "Please follow IBM Cloud dashboard and create both services.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip --version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "#Install PyWren-IBM\n",
    "!pip install -U pywren-ibm-cloud"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pywren_ibm_cloud as pywren\n",
    "pywren.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We need this to overcome Python notebooks limitations of too many open files\n",
    "import resource\n",
    "soft, hard = resource.getrlimit(resource.RLIMIT_NOFILE)\n",
    "print('Bebore:', soft, hard)\n",
    "\n",
    "# Raising the soft limit. Hard limits can be raised only by sudo users\n",
    "resource.setrlimit(resource.RLIMIT_NOFILE, (10000, hard))\n",
    "soft, hard = resource.getrlimit(resource.RLIMIT_NOFILE)\n",
    "print('After:', soft, hard)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%config Completer.use_jedi = False\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "from scipy.sparse import coo_matrix\n",
    "from collections import defaultdict\n",
    "from pyImagingMSpec.image_measures import isotope_image_correlation, isotope_pattern_match\n",
    "from cpyImagingMSpec import measure_of_chaos\n",
    "from itertools import chain\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import sys\n",
    "import io"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### IBM COS Setup\n",
    "\n",
    "Setup a bucket in IBM Cloud Object Storage\n",
    "You need an IBM COS bucket which you will use to store the input data. If you don't know of any of your existing buckets or would like like to create a new one, please navigate to your cloud resource list, then find and select your storage instance. From here, you will be able to view all your buckets and can create a new bucket in the region you prefer. Make sure you copy the correct endpoint for the bucket from the Endpoint tab of this COS service dashboard. Note: The bucket names must be unique."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fill here the bucket name you created in COS Dashboard \n",
    "#bucket_name = 'kovalev-annotation-example'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "# Define COS endpoint information. Example of US Cross Region endpoint\n",
    "#cos_endpoint = 's3.private.eu-de.cloud-object-storage.appdomain.cloud'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "logging.basicConfig(level=logging.DEBUG)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Obtain the API key and endpoint to the IBM Cloud Functions service. Navigate to Getting Started > API Key from the side menu and copy the values for \"Current Namespace\", \"Host\" and \"Key\" into the config below. Make sure to add \"https://\" to the host when adding it as the endpoint."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = {\n",
    "    'ibm_cf':  {\n",
    "        'endpoint': 'https://eu-gb.functions.cloud.ibm.com',\n",
    "        'namespace': 'kovalev@embl.de_dev', \n",
    "        'api_key': '***'\n",
    "    }, \n",
    "    'ibm_cos': {\n",
    "        'endpoint': 'https://s3.eu-gb.cloud-object-storage.appdomain.cloud',\n",
    "        'api_key' : '***'\n",
    "    },\n",
    "    'pywren' : {'storage_bucket' : 'metaspace-annotation-example'}\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "secrets = json.load(open('config.json'))\n",
    "config['ibm_cf']['api_key'] = secrets['ibm_cf']['api_key']\n",
    "config['ibm_cos']['api_key'] = secrets['ibm_cos']['api_key']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### IBM Cloud Functions setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The PyWren engine requires its server side component to be deployed in advance. This step creates a new IBM Cloud Functions function with the PyWren server side runtime. This action will be used internally by PyWren during execution phases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "runtime = 'ibmfunctions/pywren-metabolomics:3.6'\n",
    "pywren.runtime.clone_runtime(runtime, config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Upload test data into COS bucket"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ibm_boto3\n",
    "from ibm_botocore.client import Config\n",
    "from ibm_botocore.client import ClientError"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cos_client = ibm_boto3.client(service_name='s3',\n",
    "                              ibm_api_key_id=config['ibm_cos']['api_key'],\n",
    "#                               ibm_auth_endpoint=config['ibm_cos']['auth_endpoint'],\n",
    "                              config=Config(signature_version='oauth'),\n",
    "                              endpoint_url=config['ibm_cos']['endpoint'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def copy(src, target_bucket, target_key):\n",
    "    print('Copying from {} to {}/{}'.format(src, target_bucket, target_key))\n",
    "\n",
    "    with open(src, \"rb\") as fp:\n",
    "        cos_client.put_object(Bucket=target_bucket, Key=target_key, Body=fp)\n",
    "\n",
    "    print('Copy completed for {}/{}'.format(target_bucket, target_key))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "for dirpath, dirnames, filenames in os.walk('./metabolomics'):\n",
    "    for fn in filenames:\n",
    "        f_path = f'{dirpath}/{fn}'\n",
    "        copy(f_path, config['pywren']['storage_bucket'], f_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_data =   {'ds': 'metabolomics/input/ds.txt',\n",
    "                'ds_coord': 'metabolomics/input/ds_coord.txt',\n",
    "                'segments': 'metabolomics/input/segments'}\n",
    "input_db =     {'centroids': 'metabolomics/db/centroids.csv',\n",
    "                'formulas': 'metabolomics/db/formulas.csv'}\n",
    "input_config = {'config': 'metabolomics/data/config.json',\n",
    "                'meta': 'metabolomics/data/meta.json'}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Read Dataset Spectra"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_txt(key, data_stream, func):\n",
    "    rows = []\n",
    "    buffer = io.StringIO(data_stream.read().decode('utf-8'))\n",
    "    while True:\n",
    "        line = buffer.readline()\n",
    "        if not line:\n",
    "            break\n",
    "        rows.append(func(line))\n",
    "    return rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_spectrum_line(s):\n",
    "    ind_s, mzs_s, int_s = s.split('|')\n",
    "    return (int(ind_s),\n",
    "            np.fromstring(mzs_s, sep=' ').astype('float32'),\n",
    "            np.fromstring(int_s, sep=' '))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_spectrum_coord(s):\n",
    "    sp_i, x, y = map(int, s.split(','))\n",
    "    return (sp_i, x, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bucket_name = config['pywren']['storage_bucket']\n",
    "bucket_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "pw = pywren.ibm_cf_executor(config=config, runtime=runtime, runtime_memory=512)\n",
    "iterdata = [[f'{bucket_name}/{input_data[\"ds\"]}', parse_spectrum_line]]\n",
    "# NOTE: we need to be absolutely sure that using chunk_size doesn't split a line into separate chunks\n",
    "pw.map(parse_txt, iterdata, chunk_size=64*1024**2)\n",
    "results = pw.get_result()\n",
    "spectra = []\n",
    "for res_list in results:\n",
    "    spectra.extend(res_list)\n",
    "pw.clean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(spectra)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sp_i, mzs, ints = spectra[0]\n",
    "mzs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pw = pywren.ibm_cf_executor(config=config, runtime=runtime, runtime_memory=128)\n",
    "iterdata = [[f'{bucket_name}/{input_data[\"ds_coord\"]}', parse_spectrum_coord]]\n",
    "pw.map(parse_txt, iterdata)\n",
    "spectra_coords = pw.get_result()\n",
    "pw.clean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(spectra_coords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spectra_coords[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Read Molecular Database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_formulas_database(key, data_stream):\n",
    "    formulas_df = pd.read_csv(data_stream._raw_stream).set_index('formula_i')\n",
    "    return formulas_df.shape, formulas_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pw = pywren.ibm_cf_executor(config=config, runtime=runtime, runtime_memory=256)\n",
    "iterdata = [f'{bucket_name}/{input_db[\"formulas\"]}']\n",
    "pw.map(process_formulas_database, iterdata)\n",
    "formulas_shape, formulas_head = pw.get_result()\n",
    "pw.clean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "formulas_shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "formulas_head"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_centroids_database(key, data_stream):\n",
    "    centroids_df = pd.read_csv(data_stream._raw_stream).set_index('formula_i')\n",
    "    return centroids_df.shape, centroids_df.head(8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "pw = pywren.ibm_cf_executor(config=config, runtime=runtime, runtime_memory=512)\n",
    "iterdata = [f'{bucket_name}/{input_db[\"centroids\"]}']\n",
    "pw.map(process_centroids_database, iterdata)\n",
    "centroids_shape, centroids_head = pw.get_result()\n",
    "pw.clean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "centroids_shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "centroids_head"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate Ion Images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def real_pixel_indices(spectra_coords):\n",
    "    coord_pairs = [r[1:] for r in spectra_coords]\n",
    "\n",
    "    min_x, min_y = np.amin(np.asarray(coord_pairs), axis=0)\n",
    "    max_x, max_y = np.amax(np.asarray(coord_pairs), axis=0)\n",
    "\n",
    "    _coord = np.array(coord_pairs)\n",
    "    _coord = np.around(_coord, 5)  # correct for numerical precision\n",
    "    _coord -= np.amin(_coord, axis=0)\n",
    "\n",
    "    nrows, ncols = (max_y - min_y + 1,\n",
    "                    max_x - min_x + 1)\n",
    "\n",
    "    pixel_indices = _coord[:, 1] * ncols + _coord[:, 0]\n",
    "    return pixel_indices.astype(np.int32), nrows, ncols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sp_df_gen(sp_it, pixel_indices):\n",
    "    for sp_id, mzs, intensities in sp_it:\n",
    "        for mz, ints in zip(mzs, intensities):\n",
    "            yield pixel_indices[sp_id], mz, ints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gen_iso_images(spectra_it, pixel_indices, centr_df, nrows, ncols, ppm, min_px=1):\n",
    "    if len(centr_df) > 0:\n",
    "        # a bit slower than using pure numpy arrays but much shorter\n",
    "        # may leak memory because of https://github.com/pydata/pandas/issues/2659 or smth else\n",
    "        sp_df = pd.DataFrame(sp_df_gen(spectra_it, pixel_indices),\n",
    "                             columns=['idx', 'mz', 'ints']).sort_values(by='mz')\n",
    "\n",
    "        # -1, + 1 are needed to extend sf_peak_mz range so that it covers 100% of spectra\n",
    "        centr_df = centr_df[(centr_df.mz >= sp_df.mz.min() - 1) &\n",
    "                            (centr_df.mz <= sp_df.mz.max() + 1)]\n",
    "        lower = centr_df.mz.map(lambda mz: mz - mz * ppm * 1e-6)\n",
    "        upper = centr_df.mz.map(lambda mz: mz + mz * ppm * 1e-6)\n",
    "        lower_idx = np.searchsorted(sp_df.mz, lower, 'l')\n",
    "        upper_idx = np.searchsorted(sp_df.mz, upper, 'r')\n",
    "\n",
    "        for i, (l, u) in enumerate(zip(lower_idx, upper_idx)):\n",
    "            if u - l >= min_px:\n",
    "                data = sp_df.ints[l:u].values\n",
    "                if data.shape[0] > 0:\n",
    "                    idx = sp_df.idx[l:u].values\n",
    "                    row_inds = idx / ncols\n",
    "                    col_inds = idx % ncols\n",
    "                    m = coo_matrix((data, (row_inds, col_inds)), shape=(nrows, ncols), copy=True)\n",
    "                    yield centr_df.index[i], (centr_df.peak_i.iloc[i], m)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_formula_images(iso_images):\n",
    "    formula_images = defaultdict(list)\n",
    "    for formula_i, (peak_i, img) in iso_images:\n",
    "        formula_images[formula_i].append((peak_i, img))\n",
    "\n",
    "    def filter_formula_images():\n",
    "        filtered_f_images = {}\n",
    "        for f_i, images in formula_images.items():\n",
    "            images = sorted(images, key=lambda x: x[0])\n",
    "            if len(images) > 1 and images[0][0] == 0:\n",
    "                filtered_f_images[f_i] = images\n",
    "        return filtered_f_images\n",
    "\n",
    "    return filter_formula_images()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pixel_indices, nrows, ncols = real_pixel_indices(spectra_coords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pixel_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nrows, ncols"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Score Formula Images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def score_formula_images(f_images, centroids_df, empty_image):\n",
    "    formula_scores = []\n",
    "    for formula_i, images in f_images.items():\n",
    "        centr_ints = centroids_df.loc[formula_i].int.values\n",
    "\n",
    "        image_list = [empty_image] * len(centr_ints)\n",
    "        for peak_i, img in images:\n",
    "            image_list[peak_i] = img.toarray()\n",
    "        flat_image_list = [img.flat[:] for img in image_list]\n",
    "\n",
    "        m1 = isotope_pattern_match(flat_image_list, centr_ints)\n",
    "        m2 = isotope_image_correlation(flat_image_list, centr_ints[1:])\n",
    "        m3 = measure_of_chaos(image_list[0], nlevels=30)\n",
    "        formula_scores.append([formula_i, m1, m2, m3])\n",
    "\n",
    "    formula_scores_df = pd.DataFrame(formula_scores,\n",
    "                                     columns=['formula_i', 'm1', 'm2', 'm3']).set_index('formula_i')\n",
    "    formula_scores_df['msm'] = formula_scores_df.m1 * formula_scores_df.m2 * formula_scores_df.m3\n",
    "    return formula_scores_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "empty_image = np.zeros((nrows, ncols))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "empty_image.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Split Dataset into Segments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "segm_n = 256"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_segm_intervals(key, data_stream):\n",
    "    centroids_df = pd.read_csv(data_stream._raw_stream).set_index('formula_i')\n",
    "    segm_bounds_q = [i * 1 / segm_n for i in range(1, segm_n)]\n",
    "    segm_bounds = [np.quantile(centroids_df.mz.values, q) for q in segm_bounds_q]\n",
    "    segm_bounds = [0.] + segm_bounds + [sys.float_info.max]\n",
    "    segm_intervals = list(zip(segm_bounds[:-1], segm_bounds[1:]))\n",
    "    return segm_intervals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "pw = pywren.ibm_cf_executor(config=config, runtime=runtime, runtime_memory=512)\n",
    "iterdata = [f'{bucket_name}/{input_db[\"centroids\"]}']\n",
    "pw.map(get_segm_intervals, iterdata)\n",
    "segm_intervals = pw.get_result()\n",
    "pw.clean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "segm_intervals[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def iterate_over_segment(spectra, min_mz, max_mz):\n",
    "    for sp_i, mzs, ints in spectra:\n",
    "        smask = (mzs >= min_mz) & (mzs <= max_mz)\n",
    "        yield [sp_i], mzs[smask], ints[smask]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def store_segm(key, data_stream, ibm_cos, segm_i, interval):\n",
    "    spectra = parse_txt(key, data_stream, parse_spectrum_line)\n",
    "    segm_it = iterate_over_segment(spectra, *interval)\n",
    "    segm_spectra = pickle.dumps(np.array(list(segm_it)))\n",
    "    ibm_cos.put_object(Bucket=bucket_name,\n",
    "                       Key=f'{input_data[\"segments\"]}/{segm_i}.pickle',\n",
    "                       Body=segm_spectra)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "pw = pywren.ibm_cf_executor(config=config, runtime=runtime, runtime_memory=512)\n",
    "iterdata = [[f\"{bucket_name}/{input_data['ds']}\", segm_i, segm_intervals[segm_i]]\n",
    "             for segm_i in range(segm_n)]\n",
    "pw.map(store_segm, iterdata)\n",
    "results = pw.get_result()\n",
    "pw.clean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Annotation Pipeline Applied to each Segment in Parallel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_formula_images(formula_images, formula_scores_df):\n",
    "    return {f_i: images\n",
    "            for (f_i, images) in formula_images.items()\n",
    "            if f_i in formula_scores_df.index}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def annotate_segm_spectra(key, data_stream, ibm_cos):\n",
    "    print(\"Welcome to annotate spectra\")\n",
    "    centroids_stream = ibm_cos.get_object(Bucket=bucket_name, Key=input_db['centroids'])['Body']\n",
    "    print(\"Read centroids DB from IBM COS\")\n",
    "    centroids_df = pd.read_csv(centroids_stream).set_index('formula_i')\n",
    "    print(\"Create Pandas DF for centroids DB\")\n",
    "    spectra = pickle.loads(data_stream.read())\n",
    "    iso_images = gen_iso_images(spectra, pixel_indices, centroids_df, nrows, ncols, ppm=3)\n",
    "    formula_images = merge_formula_images(list(iso_images))\n",
    "    formula_scores_df = score_formula_images(formula_images, centroids_df, empty_image)\n",
    "    formula_scores_df = formula_scores_df[formula_scores_df.msm > 0]\n",
    "    formula_images = filter_formula_images(formula_images, formula_scores_df)\n",
    "    return formula_scores_df, formula_images "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "pw = pywren.ibm_cf_executor(config=config, runtime=runtime, runtime_memory=1024)\n",
    "iterdata = [f'{bucket_name}/{input_data[\"segments\"]}/{segm_i}.pickle' for segm_i in range(segm_n)]\n",
    "pw.map(annotate_segm_spectra, iterdata)\n",
    "results = pw.get_result()\n",
    "pw.clean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "formula_scores_list, formula_images_list = list(zip(*results))\n",
    "formula_scores_df = pd.concat(formula_scores_list)\n",
    "formula_images = dict(chain(*[segm_formula_images.items()\n",
    "                              for segm_formula_images in formula_images_list]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img = formula_images[896952][0][1]\n",
    "plt.imshow(img.toarray())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clean Segments Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_segments_datasets(bucket, key, data_stream, ibm_cos):\n",
    "    ibm_cos.delete_object(Bucket=bucket, Key=key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "pw = pywren.ibm_cf_executor(config=config, runtime=runtime, runtime_memory=256)\n",
    "data_stream = f'{bucket_name}/{input_data[\"segments\"]}'\n",
    "pw.map(clean_segments_datasets, data_stream)\n",
    "pw.get_result()\n",
    "pw.clean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
